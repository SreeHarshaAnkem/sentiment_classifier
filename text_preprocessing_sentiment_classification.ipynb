{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import string\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(files):\n",
    "    '''\n",
    "        This function reads text files from a directory and creates a list of documents(corpus).\n",
    "    '''\n",
    "    docs = []\n",
    "    with open(files, \"r\") as doc:\n",
    "        text = doc.read()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(document, print_n=1000, label=None):\n",
    "\n",
    "    import spacy\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    l_stop_words = stopwords.words('english')\n",
    "    document = document.lower()\n",
    "    doc = nlp(document)\n",
    "    lemmas = [re.sub('[^a-zA-Z]', '', token.lemma_) for token in doc \n",
    "              if token.lemma_ != \"-PRON-\" and\n",
    "              token.lemma_ not in l_stop_words and\n",
    "              token.lemma_ not in string.punctuation]\n",
    "    lemmas = [lemma for lemma in lemmas if lemma!=\"\"]\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_tokens(series):\n",
    "    from collections import Counter\n",
    "    from functools import reduce\n",
    "\n",
    "    tokens = reduce(lambda x, y: x+y, series)\n",
    "    token_count = Counter(tokens)\n",
    "    df_token_count = pd.DataFrame.from_dict(\n",
    "        token_count, orient=\"index\").reset_index()\n",
    "    df_token_count.rename(columns={\"index\": \"token\",\n",
    "                                   0: \"count\"}, inplace=True)\n",
    "    overall_count = df_token_count[\"count\"].sum()\n",
    "    df_token_count[\"token_share\"] = df_token_count[\"count\"]/overall_count\n",
    "    df_token_count[\"token_cum_share\"] = df_token_count[\"token_share\"].cumsum()\n",
    "    frequent_tokens = df_token_count.loc[df_token_count[\"token_cum_share\"] <= 0.75, \"token\"].unique()\n",
    "    return frequent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import pickle\n",
    "    # Read all file names\n",
    "    data_path = \"data/aclImdb/train/\"\n",
    "    pos_files = [os.path.join(\"data/aclImdb/train/pos\", f)\n",
    "                 for f in os.listdir(\"data/aclImdb/train/pos\")]\n",
    "    neg_files = [os.path.join(\"data/aclImdb/train/neg\", f)\n",
    "                 for f in os.listdir(\"data/aclImdb/train/neg\")]\n",
    "    logging.info(\"Read all file names...\")\n",
    "\n",
    "    # Read text from all files\n",
    "    with mp.Pool() as pool:\n",
    "        pos_reviews = pool.map(read_text, pos_files)\n",
    "        neg_reviews = pool.map(read_text, neg_files)\n",
    "    logging.info(\"Read text from all files...\")\n",
    "\n",
    "    # Preprocess text\n",
    "    with mp.Pool() as pool:\n",
    "        logging.info(\"Preprocessing Positive Reviews...\")\n",
    "        pos_corpus_processed = pool.map(preprocess_text, pos_reviews)\n",
    "        logging.info(\"Preprocessing Negative Reviews...\")\n",
    "        neg_corpus_processed = pool.map(preprocess_text, neg_reviews)\n",
    "    logging.info(\"Preprocessed text...\")\n",
    "\n",
    "    # Create Dataframe from preprocessed text\n",
    "    df_pos = pd.DataFrame()\n",
    "    df_pos[\"corpus\"] = pos_reviews\n",
    "    df_pos[\"processed\"] = pos_corpus_processed\n",
    "    df_pos[\"sentiment\"] = 1\n",
    "\n",
    "    df_neg = pd.DataFrame()\n",
    "    df_neg[\"corpus\"] = neg_reviews\n",
    "    df_neg[\"processed\"] = neg_corpus_processed\n",
    "    df_neg[\"sentiment\"] = 0\n",
    "\n",
    "    df_reviews = pd.concat([df_pos, df_neg], axis=0)\n",
    "    logging.info(df_reviews.head())\n",
    "    logging.info(\"Data frame with preprocessed text created...\")\n",
    "\n",
    "    # Retain only frequent tokens\n",
    "    frequent_tokens = get_frequent_tokens(df_reviews[\"processed\"].values)\n",
    "    logging.info(\"No. of frequent tokens: {}\".format(len(frequent_tokens)))\n",
    "    df_reviews[\"processed\"] = df_reviews[\"processed\"].apply(\n",
    "        lambda review: [token for token in review if token in frequent_tokens])\n",
    "    logging.info(\"Retaining only frequent tokens...\")\n",
    "\n",
    "    # Create token idx mapping\n",
    "    dict_frequent_tokens_map = {token: idx +\n",
    "                                1 for idx, token in enumerate(frequent_tokens)}\n",
    "    logging.info(\"Creating token idx mapping...\")\n",
    "\n",
    "    # Create idx mapping\n",
    "    df_reviews[\"processed_idx\"] = df_reviews[\"processed\"].apply(\n",
    "        lambda review: [dict_frequent_tokens_map[token] for token in review])\n",
    "    logging.info(\"Creating idx sequences...\")\n",
    "\n",
    "    # Save token-idx map to disk\n",
    "    with open(\"data/processed_data/token_idx_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict_frequent_tokens_map, f)\n",
    "    logging.info(\"Saving token idx mapping to disk...\")\n",
    "\n",
    "    # Save processed data to de\n",
    "    df_reviews.to_pickle(\"data/processed_data/train_processed_data.pkl\")\n",
    "    logging.info(\"Saving preprocessed data to disk...\")\n",
    "\n",
    "    \"\"\"\n",
    "    Read this way:\n",
    "    with open('token_idx_mapping.pkl', 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run train_sentiment_classifer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_frequent_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
